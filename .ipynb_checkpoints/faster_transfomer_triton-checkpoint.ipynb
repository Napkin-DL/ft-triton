{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f068ef27",
   "metadata": {},
   "source": [
    "## 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b0dd2",
   "metadata": {},
   "source": [
    "도커 이미지 경로를 EBS로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "echo '{\n",
    "    \"runtimes\": {\n",
    "        \"nvidia\": {\n",
    "            \"path\": \"nvidia-container-runtime\",\n",
    "            \"runtimeArgs\": []\n",
    "        }\n",
    "    }\n",
    "}' > daemon.json\n",
    "\n",
    "sudo cp daemon.json /etc/docker/daemon.json && rm daemon.json\n",
    "\n",
    "DAEMON_PATH=\"/etc/docker\"\n",
    "MEMORY_SIZE=10G\n",
    "\n",
    "FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# echo $FLAG\n",
    "\n",
    "if [ \"$FLAG\" == true ]; then\n",
    "    echo \"Already revised\"\n",
    "else\n",
    "    echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "    sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "    sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "    sudo service docker restart\n",
    "    echo \"Docker Restart\"\n",
    "fi\n",
    "\n",
    "sudo docker info | grep Root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabac7de",
   "metadata": {},
   "source": [
    "based on https://github.com/triton-inference-server/fastertransformer_backend/tree/dev/v1.1_beta\n",
    "\n",
    "**[주의] Master branch가 아니라 dev/v1.1_beta branch를 클론해야 함**\n",
    "```\n",
    "git clone --single-branch --branch dev/v1.1_beta https://github.com/triton-inference-server/fastertransformer_backend/\n",
    "```\n",
    "\n",
    "* 모델 생성과 local mode 테스트를 위해 ml.p3.16xlarge 노트북 인스턴스에서 작업\n",
    "* 이미지 크기가 크므로 노트북 생성시 디스크 용량 증가 및 docker image 경로 변경 필요\n",
    "* fastertransformer_backend README 참고하여 git clone(fastertransformer_backend, triton, FasterTransformer)\n",
    "\n",
    "SageMaker Triton image pull(us-east-1 기준)\n",
    "* ECR 로긴 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_image_account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}\n",
    "\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "triton_image_account_id = triton_image_account_id_map[region]\n",
    "triton_image_uri = f'{triton_image_account_id}.dkr.ecr.{region}.amazonaws.com/sagemaker-tritonserver:21.08-py3'\n",
    "print(triton_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be6b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s \"$region\" \"$triton_image_account_id\" \"$triton_image_uri\"\n",
    "$(aws ecr get-login --no-include-email --registry-ids $2 --region $1)\n",
    "docker pull $3\n",
    "docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cb0ba",
   "metadata": {},
   "source": [
    "Image build & push\n",
    "* [Dockerfile](https://github.com/triton-inference-server/fastertransformer_backend/blob/dev/v1.1_beta/docker/Dockerfile)에서 Base Image를 SageMaker Triton 이미지로 교체하고 마지막에 serve 파일을 대체.\n",
    "* serve 파일은 [원본](https://github.com/triton-inference-server/server/blob/main/docker/sagemaker/serve)에서 마지막 실행 명령만 faster transformer 백엔드의 실행 명령을 참고하여 수정했음.\n",
    "* 원래 dockerfile이 있는 경로(workspace/fastertransformer_backend/docker)에 Dockerfile.sm을 붙여넣고,\n",
    "* 상위 폴더(workspace/fastertransformer_backend)에 serve파일 붙여 넣은 후 docker build(터미널에서)\n",
    "\n",
    "```\n",
    "docker build -t {account_number}.dkr.ecr.us-east-1.amazonaws.com/sm-triton-ft:21.08-py3 -f docker/Dockerfile.sm .\n",
    "```\n",
    "\n",
    "* Push 전에 ECR 레포지토리 sm-triton-ft 생성, ECR 로긴, push 권한 설정 필요\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "algorithm_name=sm-triton-ft\n",
    "account_number=$(aws sts get-caller-identity --query Account --output text)\n",
    "region=$(aws configure get region)\n",
    "#version='21.08-py3'\n",
    "version='latest'\n",
    "fullname=\"${account_number}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:${version}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "echo \"[Note] Please copy the command below and run it in the terminal. You can run it directly in jupyter notebook, but it is recommended to run it in a terminal for debugging.\"\n",
    "echo \"\"\n",
    "echo \"docker build -t ${account_number}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:${version} -f docker/Dockerfile.sm .\"\n",
    "echo \"docker push ${account_number}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:${version}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef44d990",
   "metadata": {},
   "source": [
    "## Model 생성 및 S3 업로드\n",
    "\n",
    "모델 생성\n",
    "* [fastertransformer_backend README.md How to set the model configuration](https://github.com/triton-inference-server/fastertransformer_backend/tree/dev/v1.1_beta#how-to-set-the-model-configuration) 참고 Prepare Triton GPT model store\n",
    "\n",
    "config.pbtxt 수정\n",
    "* tensor_para_size = 8\n",
    "* model_checkpoint_path = \"/opt/ml/model/fastertransformer/1/8-gpu\"\n",
    "\n",
    "모델 압축 및 S3 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2121e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json -P models\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt -P models\n",
    "!wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O megatron_lm_345m_v0.0.zip\n",
    "!mkdir -p ./models/megatron-models/345m\n",
    "!unzip megatron_lm_345m_v0.0.zip -d models/megatron-models/345m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-ft/fastertransformer/\n",
    "!cp -r ./fastertransformer_backend/all_models/ triton-serve-ft/fastertransformer/1/\n",
    "!cp config.pbtxt triton-serve-ft/fastertransformer\n",
    "!tar -C triton-serve-ft/ -czf model.tar.gz fastertransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdacac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=\"triton-serve-ft\")\n",
    "image_uri = \"{account_number}.dkr.ecr.us-east-1.amazonaws.com/sm-triton-ft:21.08-py3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd816e62",
   "metadata": {},
   "source": [
    "## Local mode test\n",
    "\n",
    "* [fastertransformer_backend Run Serving on Single Node](https://github.com/triton-inference-server/fastertransformer_backend/tree/dev/v1.1_beta#run-serving-on-single-node) 참고\n",
    "* [SageMaker Triton example](https://github.com/aws/amazon-sagemaker-examples/blob/1072934944e5270f7f2fb0d9e0e1a86ce96aa57e/sagemaker-triton/nlp_bert/triton_nlp_bert.ipynb) 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = \"triton-ft-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "model = sagemaker.model.Model(image_uri=image_uri, model_data=model_uri, role=role, \n",
    "                              name=sm_model_name)\n",
    "model.deploy(initial_instance_count=1, instance_type='local_gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "local_sess = LocalSession()\n",
    "predictor = Predictor(\n",
    "    endpoint_name=model.endpoint_name, \n",
    "    sagemaker_session=local_sess,\n",
    "    serializer=JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e936c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "from tritonclientutils import np_to_triton_dtype, InferenceServerException\n",
    "\n",
    "input_start_ids = np.array([\n",
    "    [9915, 27221, 59, 77, 383, 1853, 3327, 1462],\n",
    "    [6601, 4237, 345, 460, 779, 284, 787, 257],\n",
    "    [59, 77, 611, 7, 9248, 796, 657, 8],\n",
    "    [38, 10128, 6032, 651, 8699, 4, 4048, 20753],\n",
    "    [21448, 7006, 930, 12901, 930, 7406, 7006, 198],\n",
    "    [13256, 11, 281, 1605, 3370, 11, 1444, 6771],\n",
    "    [9915, 27221, 59, 77, 383, 1853, 3327, 1462],\n",
    "    [6601, 4237, 345, 460, 779, 284, 787, 257]\n",
    "], np.uint32)\n",
    "input_start_ids = input_start_ids.reshape([input_start_ids.shape[0], 1, input_start_ids.shape[1]])\n",
    "input_data = np.tile(input_start_ids, (1, 1, 1))\n",
    "input_len = np.array([[sentence.size] for sentence in input_start_ids], np.uint32)\n",
    "output_len = np.ones_like(input_len).astype(np.uint32) * 24\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT_ID\", \"shape\": input_data.shape, \"datatype\": np_to_triton_dtype(input_data.dtype), \n",
    "         \"data\": input_data.tolist()},\n",
    "        {\"name\": \"REQUEST_INPUT_LEN\", \"shape\": input_len.shape, \"datatype\": np_to_triton_dtype(input_len.dtype), \n",
    "         \"data\": input_len.tolist()},\n",
    "        {\"name\": \"REQUEST_OUTPUT_LEN\", \"shape\": output_len.shape, \"datatype\": np_to_triton_dtype(output_len.dtype),\n",
    "         \"data\": output_len.tolist()}\n",
    "    ]\n",
    "}\n",
    "request_parallelism = 100\n",
    "for i in range(request_parallelism):\n",
    "    predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bad5e7",
   "metadata": {},
   "source": [
    "## 엔드포인트, 모델 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()\n",
    "# model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a028cba1",
   "metadata": {},
   "source": [
    "## Endpoint 생성 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = \"triton-ft-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1cbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"triton-ft-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.p3.16xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"triton-ft-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "\n",
    "input_start_ids = np.array([\n",
    "    [9915, 27221, 59, 77, 383, 1853, 3327, 1462],\n",
    "    [6601, 4237, 345, 460, 779, 284, 787, 257],\n",
    "    [59, 77, 611, 7, 9248, 796, 657, 8],\n",
    "    [38, 10128, 6032, 651, 8699, 4, 4048, 20753],\n",
    "    [21448, 7006, 930, 12901, 930, 7406, 7006, 198],\n",
    "    [13256, 11, 281, 1605, 3370, 11, 1444, 6771],\n",
    "    [9915, 27221, 59, 77, 383, 1853, 3327, 1462],\n",
    "    [6601, 4237, 345, 460, 779, 284, 787, 257]\n",
    "], np.uint32)\n",
    "input_start_ids = input_start_ids.reshape([input_start_ids.shape[0], 1, input_start_ids.shape[1]])\n",
    "input_data = np.tile(input_start_ids, (1, 1, 1))\n",
    "input_len = np.array([[sentence.size] for sentence in input_start_ids], np.uint32)\n",
    "output_len = np.ones_like(input_len).astype(np.uint32) * 24\n",
    "\n",
    "from datetime import datetime\n",
    "request_parallelism = 10\n",
    "\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "stop_time = datetime.now()\n",
    "latency = ((stop_time - start_time).total_seconds()* 1000.0 / request_parallelism)\n",
    "\n",
    "for i in range(request_parallelism):\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"name\": \"INPUT_ID\", \"shape\": input_data.shape, \"datatype\": np_to_triton_dtype(input_data.dtype), \n",
    "             \"data\": input_data.tolist()},\n",
    "            {\"name\": \"REQUEST_INPUT_LEN\", \"shape\": input_len.shape, \"datatype\": np_to_triton_dtype(input_len.dtype), \n",
    "             \"data\": input_len.tolist()},\n",
    "            {\"name\": \"REQUEST_OUTPUT_LEN\", \"shape\": output_len.shape, \"datatype\": np_to_triton_dtype(output_len.dtype),\n",
    "             \"data\": output_len.tolist()}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    "    )\n",
    "\n",
    "    print(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n",
    "print(f\"[INFO] execution time: {latency} ms\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967b94c",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
