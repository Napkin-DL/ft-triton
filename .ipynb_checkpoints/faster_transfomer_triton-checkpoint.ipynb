{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e62eac9",
   "metadata": {},
   "source": [
    "## 1. 준비\n",
    "\n",
    "based on https://github.com/triton-inference-server/fastertransformer_backend/tree/dev/v1.1_beta\n",
    "\n",
    "* 모델 생성과 local mode 테스트를 위해 ml.p3.16xlarge 노트북 인스턴스에서 작업\n",
    "* 이미지 크기가 크므로 노트북 생성시 디스크 용량 증가 및 docker image 경로 변경 필요\n",
    "* fastertransformer_backend README 참고하여 git clone(fastertransformer_backend, triton, FasterTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e024e",
   "metadata": {},
   "source": [
    "### 도커 이미지 경로를 EBS로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "echo '{\n",
    "    \"runtimes\": {\n",
    "        \"nvidia\": {\n",
    "            \"path\": \"nvidia-container-runtime\",\n",
    "            \"runtimeArgs\": []\n",
    "        }\n",
    "    }\n",
    "}' > daemon.json\n",
    "\n",
    "sudo cp daemon.json /etc/docker/daemon.json && rm daemon.json\n",
    "\n",
    "DAEMON_PATH=\"/etc/docker\"\n",
    "MEMORY_SIZE=10G\n",
    "\n",
    "FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# echo $FLAG\n",
    "\n",
    "if [ \"$FLAG\" == true ]; then\n",
    "    echo \"Already revised\"\n",
    "else\n",
    "    echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "    sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "    sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "    sudo service docker restart\n",
    "    echo \"Docker Restart\"\n",
    "fi\n",
    "\n",
    "sudo docker info | grep Root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c7012",
   "metadata": {},
   "source": [
    "### SageMaker Triton image pull(us-east-1 기준)\n",
    "* ECR 로그인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_image_account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}\n",
    "\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "triton_image_account_id = triton_image_account_id_map[region]\n",
    "triton_image_uri = f'{triton_image_account_id}.dkr.ecr.{region}.amazonaws.com/sagemaker-tritonserver:21.08-py3'\n",
    "print(triton_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s \"$region\" \"$triton_image_account_id\" \"$triton_image_uri\"\n",
    "$(aws ecr get-login --no-include-email --registry-ids $2 --region $1)\n",
    "docker pull $3\n",
    "docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280cd8f",
   "metadata": {},
   "source": [
    "### git clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b34c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "git clone https://github.com/triton-inference-server/fastertransformer_backend.git -b dev/v1.1_beta\n",
    "git clone https://github.com/triton-inference-server/server.git # We need some tools when we test this backend\n",
    "git clone -b dev/v5.0_beta https://github.com/NVIDIA/FasterTransformer # Used for convert the checkpoint and triton output\n",
    "ln -s server/qa/common .\n",
    "cp serve config.pbtxt fastertransformer_backend && cp Dockerfile.sm fastertransformer_backend/docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12cd51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "algorithm_name = \"sm-triton-ft\"\n",
    "#version = \"latest\"\n",
    "version = \"21.08-al2-py3\"\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{algorithm_name}:{version}\"\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60091c",
   "metadata": {},
   "source": [
    "### ECR 로그인 & 도커 이미지 build 및 push\n",
    "\n",
    "* [Dockerfile](https://github.com/triton-inference-server/fastertransformer_backend/blob/dev/v1.1_beta/docker/Dockerfile)에서 Base Image를 SageMaker Triton 이미지로 교체하고 마지막에 serve 파일을 대체.\n",
    "* serve 파일은 [원본](https://github.com/triton-inference-server/server/blob/main/docker/sagemaker/serve)에서 마지막 실행 명령만 faster transformer 백엔드의 실행 명령을 참고하여 수정했음.\n",
    "* 원래 dockerfile이 있는 경로(workspace/fastertransformer_backend/docker)에 Dockerfile.sm을 붙여넣고,\n",
    "* 상위 폴더(workspace/fastertransformer_backend)에 serve파일 붙여 넣은 후 docker build(터미널에서)\n",
    "\n",
    "```\n",
    "docker build -t {account_number}.dkr.ecr.us-east-1.amazonaws.com/sm-triton-ft:21.08-py3 -f docker/Dockerfile.sm .\n",
    "```\n",
    "\n",
    "* Push 전에 ECR 레포지토리 sm-triton-ft 생성, ECR 로긴, push 권한 설정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5fb1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s \"$algorithm_name\" \"$region\" \"$image_uri\"\n",
    "\n",
    "algorithm_name=$1\n",
    "region=$2\n",
    "fullname=$3\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin $3\n",
    "\n",
    "echo \"[Note] Please copy the command below and run it in the terminal. You can run it directly in jupyter notebook, but it is recommended to run it in a terminal for debugging.\"\n",
    "echo \"\"\n",
    "echo \"cd /home/ec2-user/SageMaker/ft-triton/fastertransformer_backend && docker build -t ${fullname} -f docker/Dockerfile.sm .\"\n",
    "echo \"docker push ${fullname}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f980f757",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Model 생성 및 S3 업로드\n",
    "\n",
    "모델 생성\n",
    "* [fastertransformer_backend README.md How to set the model configuration](https://github.com/triton-inference-server/fastertransformer_backend/tree/dev/v1.1_beta#how-to-set-the-model-configuration) 참고 Prepare Triton GPT model store\n",
    "\n",
    "config.pbtxt 수정\n",
    "* tensor_para_size = 8\n",
    "* model_checkpoint_path = \"/opt/ml/model/fastertransformer/1/8-gpu\"\n",
    "\n",
    "모델 압축 및 S3 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "WORKSPACE = os.getcwd()\n",
    "SRC_MODELS_DIR = f\"{WORKSPACE}/models\"\n",
    "TRITON_MODELS_STORE = f\"{WORKSPACE}/triton-model-store\"\n",
    "TRITON_DOCKER_IMAGE = image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93121e5",
   "metadata": {},
   "source": [
    "### 모델 다운로드 (Megatron GPT-3 345M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json -P models\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt -P models\n",
    "!wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O megatron_lm_345m_v0.0.zip\n",
    "!mkdir -p {SRC_MODELS_DIR}/megatron-models/345m\n",
    "!unzip megatron_lm_345m_v0.0.zip -d models/megatron-models/345m\n",
    "!mkdir {TRITON_MODELS_STORE}/fastertransformer/1 -p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641d31e",
   "metadata": {},
   "source": [
    "### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGUMENT_STR = f\"-i {SRC_MODELS_DIR}/megatron-models/345m/release/ \" + \\\n",
    "    f\"-o {TRITON_MODELS_STORE}/fastertransformer/1 \" + \\\n",
    "    \"-trained_gpu_num 1 -infer_gpu_num 8 -head_num 16\"\n",
    "\n",
    "!echo {ARGUMENT_STR}\n",
    "\n",
    "!docker run --rm -it --gpus=all \\\n",
    "    -e SRC_MODELS_DIR={SRC_MODELS_DIR} \\\n",
    "    -e TRITON_MODELS_STORE={TRITON_MODELS_STORE} \\\n",
    "    -v {WORKSPACE}:{WORKSPACE} \\\n",
    "    {TRITON_DOCKER_IMAGE} \\\n",
    "    bash -c \"python {WORKSPACE}/FasterTransformer/examples/pytorch/gpt/utils/megatron_ckpt_convert.py {ARGUMENT_STR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47122608",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_model_path = f'{TRITON_MODELS_STORE}/fastertransformer/1/'\n",
    "!ls {converted_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13aff0",
   "metadata": {},
   "source": [
    "### 모델 압축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "serve_dir = \"triton-serve-ft\"\n",
    "!rm -rf {serve_dir} && mkdir -p {serve_dir}/fastertransformer/\n",
    "!cp -r {converted_model_path} {serve_dir}/fastertransformer/1/\n",
    "!cp config.pbtxt {serve_dir}/fastertransformer\n",
    "!tar -C {serve_dir}/ -czf model.tar.gz fastertransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40782123",
   "metadata": {},
   "source": [
    "### 모델 S3 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d20160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=serve_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545116c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Local mode test\n",
    "\n",
    "* [fastertransformer_backend Run Serving on Single Node](https://github.com/triton-inference-server/fastertransformer_backend/tree/dev/v1.1_beta#run-serving-on-single-node) 참고\n",
    "* [SageMaker Triton example](https://github.com/aws/amazon-sagemaker-examples/blob/1072934944e5270f7f2fb0d9e0e1a86ce96aa57e/sagemaker-triton/nlp_bert/triton_nlp_bert.ipynb) 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a88f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5114ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_local_model_name = \"triton-ft-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "model = sagemaker.model.Model(image_uri=image_uri, model_data=model_uri, role=role, \n",
    "                              name=sm_local_model_name)\n",
    "model.deploy(initial_instance_count=1, instance_type='local_gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "local_sess = LocalSession()\n",
    "predictor = Predictor(\n",
    "    endpoint_name=model.endpoint_name, \n",
    "    sagemaker_session=local_sess,\n",
    "    serializer=JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import np_to_triton_dtype, InferenceServerException\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def make_payload(tokenizer, decode_length=24):\n",
    "    max_length = 4\n",
    "    num_samples = 4\n",
    "    \n",
    "    sample1 = tokenizer(\"Machine Learning skills require\", max_length=max_length, truncation=True)['input_ids']\n",
    "    sample2 = tokenizer(\"Study, play,\", max_length=max_length, truncation=True)['input_ids']\n",
    "    sample3 = tokenizer(\"Amazon's biggest success\", max_length=max_length, truncation=True)['input_ids']\n",
    "    sample4 = tokenizer(\"Amazon SageMaker is\", max_length=max_length, truncation=True)['input_ids']    \n",
    "    input_start_ids = np.array([sample1, sample2, sample3, sample4], np.uint32)\n",
    "\n",
    "    input_start_ids = input_start_ids.reshape([input_start_ids.shape[0], 1, input_start_ids.shape[1]])\n",
    "    input_data = np.tile(input_start_ids, (1, 1, 1))\n",
    "    input_len = np.array([[sentence.size] for sentence in input_start_ids], np.uint32)\n",
    "    output_len = np.ones_like(input_len).astype(np.uint32) * decode_length\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"name\": \"INPUT_ID\", \"shape\": input_data.shape, \"datatype\": np_to_triton_dtype(input_data.dtype), \n",
    "             \"data\": input_data.tolist()},\n",
    "            {\"name\": \"REQUEST_INPUT_LEN\", \"shape\": input_len.shape, \"datatype\": np_to_triton_dtype(input_len.dtype), \n",
    "             \"data\": input_len.tolist()},\n",
    "            {\"name\": \"REQUEST_OUTPUT_LEN\", \"shape\": output_len.shape, \"datatype\": np_to_triton_dtype(output_len.dtype),\n",
    "             \"data\": output_len.tolist()}\n",
    "        ]\n",
    "    }    \n",
    "    \n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "payload = make_payload(tokenizer, decode_length=24)\n",
    "\n",
    "num_infers = 2\n",
    "latency = np.zeros(num_infers)\n",
    "for i in range(num_infers):\n",
    "    start_time = datetime.now()     \n",
    "    outputs = predictor.predict(payload)\n",
    "    stop_time = datetime.now()\n",
    "    delta = ((stop_time - start_time).total_seconds()* 1000.0)\n",
    "    latency[i] = delta\n",
    "\n",
    "avg = np.average(latency)\n",
    "p50 = np.quantile(latency, 0.50)\n",
    "p95 = np.quantile(latency, 0.95) \n",
    "p99 = np.quantile(latency, 0.99)\n",
    "print(f'avg latency: {avg:.4f} ms')    \n",
    "print(f'p50 latency: {p50:.4f} ms')\n",
    "print(f'p95 latency: {p95:.4f} ms')\n",
    "print(f'p99 latency: {p99:.4f} ms')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_json = json.loads(outputs)['outputs'][0]\n",
    "num_output_samples = output_json['shape'][0]\n",
    "#print(output_json['shape']) # num_batches x 1 x (input length + decode length)\n",
    "output_tokens = output_json['data']\n",
    "output_tokens = np.reshape(output_tokens, (num_output_samples, -1))\n",
    "\n",
    "for k in range(num_output_samples):\n",
    "    text = tokenizer.decode(output_tokens[k], clean_up_tokenization_spaces=True)\n",
    "    print(f'[Output sample {k+1}]')\n",
    "    print(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6795d",
   "metadata": {},
   "source": [
    "### 엔드포인트, 모델 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99298625",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a380d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Endpoint 생성 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c9b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = \"triton-ft-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"triton-ft-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.p3.16xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8852a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"triton-ft-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64045c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "def make_endpoint_link(region, endpoint_name, endpoint_task):\n",
    "    endpoint_link = f'<b><a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\">{endpoint_task} Review Endpoint</a></b>'   \n",
    "    return endpoint_link \n",
    "        \n",
    "endpoint_link = make_endpoint_link(region, endpoint_name, '[Deploy model from S3]')\n",
    "display(HTML(endpoint_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.Session().wait_for_endpoint(endpoint_name, poll=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ed619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def benchmark(sm_runtime_client, endpoint_name, payload, num_infers=20):\n",
    "    latency = np.zeros(num_infers)\n",
    "    t = tqdm(range(num_infers), position=0, leave=True)\n",
    "\n",
    "    for i in t:\n",
    "        start_time = datetime.now()          \n",
    "        response = client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    "        )\n",
    "        outputs = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "        stop_time = datetime.now()\n",
    "        delta = ((stop_time - start_time).total_seconds()* 1000.0)\n",
    "        latency[i] = delta  \n",
    "        \n",
    "    avg = np.average(latency)\n",
    "    p50 = np.quantile(latency, 0.50)\n",
    "    p95 = np.quantile(latency, 0.95) \n",
    "    p99 = np.quantile(latency, 0.99)\n",
    "    print(f'avg latency: {avg:.4f} ms')    \n",
    "    print(f'p50 latency: {p50:.4f} ms')\n",
    "    print(f'p95 latency: {p95:.4f} ms')\n",
    "    print(f'p99 latency: {p99:.4f} ms')\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38a5ec",
   "metadata": {},
   "source": [
    "GPT 모델은 auto-regressive 모델이므로 decode_length가 늘어날수록 latency가 증가함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603238f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = make_payload(tokenizer, decode_length=24)\n",
    "outputs = benchmark(client, endpoint_name, payload, num_infers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff835289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = outputs['outputs'][0]\n",
    "num_output_samples = output_json['shape'][0]\n",
    "#print(output_json['shape']) # num_batches x 1 x (input length + decode length)\n",
    "output_tokens = output_json['data']\n",
    "output_tokens = np.reshape(output_tokens, (num_output_samples, -1))\n",
    "\n",
    "for k in range(num_output_samples):\n",
    "    text = tokenizer.decode(output_tokens[k], clean_up_tokenization_spaces=True)\n",
    "    print(f'[Output sample {k+1}]')\n",
    "    print(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d7479d",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo rm -rf common fastertransformer_backend FasterTransformer server triton-model-store triton-serve-ft models models.tar.gz\n",
    "!rm megatron_lm_345m_v0.0.zip model.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
