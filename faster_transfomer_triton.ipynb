{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3273c2d",
   "metadata": {},
   "source": [
    "## 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e2a447",
   "metadata": {},
   "source": [
    "based on https://github.com/triton-inference-server/fastertransformer_backend/tree/dev/v1.1_beta\n",
    "\n",
    "* 모델 생성과 local mode 테스트를 위해 ml.p3.16xlarge 노트북 인스턴스에서 작업\n",
    "* 이미지 크기가 크므로 노트북 생성시 디스크 용량 증가 및 docker image 경로 변경 필요\n",
    "* fastertransformer_backend README 참고하여 git clone(fastertransformer_backend, triton, FasterTransformer)\n",
    "\n",
    "SageMaker Triton image pull(us-east-1 기준)\n",
    "* ECR 로긴 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86436e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull 785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:21.08-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5af74f",
   "metadata": {},
   "source": [
    "Image build & push\n",
    "* [Dockerfile](https://github.com/triton-inference-server/fastertransformer_backend/blob/dev/v1.1_beta/docker/Dockerfile)에서 Base Image를 SageMaker Triton 이미지로 교체하고 마지막에 serve 파일을 대체.\n",
    "* serve 파일은 [원본](https://github.com/triton-inference-server/server/blob/main/docker/sagemaker/serve)에서 마지막 실행 명령만 faster transformer 백엔드의 실행 명령을 참고하여 수정했음.\n",
    "* 원래 dockerfile이 있는 경로(workspace/fastertransformer_backend/docker)에 Dockerfile.sm을 붙여넣고,\n",
    "* 상위 폴더(workspace/fastertransformer_backend)에 serve파일 붙여 넣은 후 docker build(터미널에서)\n",
    "\n",
    "```\n",
    "docker build -t {account_number}.dkr.ecr.us-east-1.amazonaws.com/sm-triton-ft:21.08-py3 -f docker/Dockerfile.sm .\n",
    "```\n",
    "\n",
    "* Push 전에 ECR 레포지토리 sm-triton-ft 생성, ECR 로긴, push 권한 설정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19156d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push {account_number}.dkr.ecr.us-east-1.amazonaws.com/sm-triton-ft:21.08-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83152d",
   "metadata": {},
   "source": [
    "## Model 생성 및 S3 업로드\n",
    "\n",
    "모델 생성\n",
    "* [fastertransformer_backend README.md How to set the model configuration](https://github.com/triton-inference-server/fastertransformer_backend/tree/dev/v1.1_beta#how-to-set-the-model-configuration) 참고 Prepare Triton GPT model store\n",
    "\n",
    "config.pbtxt 수정\n",
    "* tensor_para_size = 8\n",
    "* model_checkpoint_path = \"/opt/ml/model/fastertransformer/1/8-gpu\"\n",
    "\n",
    "모델 압축 및 S3 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json -P models\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt -P models\n",
    "!wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O megatron_lm_345m_v0.0.zip\n",
    "!mkdir -p ./models/megatron-models/345m\n",
    "!unzip megatron_lm_345m_v0.0.zip -d models/megatron-models/345m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-ft/fastertransformer/\n",
    "!cp -r ./fastertransformer_backend/all_models/ triton-serve-ft/fastertransformer/1/\n",
    "!cp config.pbtxt triton-serve-ft/fastertransformer\n",
    "!tar -C triton-serve-ft/ -czf model.tar.gz fastertransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81143a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e426b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=\"triton-serve-ft\")\n",
    "image_uri = \"{account_number}.dkr.ecr.us-east-1.amazonaws.com/sm-triton-ft:21.08-py3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2bd736",
   "metadata": {},
   "source": [
    "## Local mode test\n",
    "\n",
    "* [fastertransformer_backend Run Serving on Single Node](https://github.com/triton-inference-server/fastertransformer_backend/tree/dev/v1.1_beta#run-serving-on-single-node) 참고\n",
    "* [SageMaker Triton example](https://github.com/aws/amazon-sagemaker-examples/blob/1072934944e5270f7f2fb0d9e0e1a86ce96aa57e/sagemaker-triton/nlp_bert/triton_nlp_bert.ipynb) 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda88a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = \"triton-ft-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "model = sagemaker.model.Model(image_uri=image_uri, model_data=model_uri, role=role, \n",
    "                              name=sm_model_name)\n",
    "model.deploy(initial_instance_count=1, instance_type='local_gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1843103",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65986df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "local_sess = LocalSession()\n",
    "predictor = Predictor(\n",
    "    endpoint_name=model.endpoint_name, \n",
    "    sagemaker_session=local_sess,\n",
    "    serializer=JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "from tritonclientutils import np_to_triton_dtype, InferenceServerException\n",
    "\n",
    "input_start_ids = np.array([\n",
    "    [9915, 27221, 59, 77, 383, 1853, 3327, 1462],\n",
    "    [6601, 4237, 345, 460, 779, 284, 787, 257],\n",
    "    [59, 77, 611, 7, 9248, 796, 657, 8],\n",
    "    [38, 10128, 6032, 651, 8699, 4, 4048, 20753],\n",
    "    [21448, 7006, 930, 12901, 930, 7406, 7006, 198],\n",
    "    [13256, 11, 281, 1605, 3370, 11, 1444, 6771],\n",
    "    [9915, 27221, 59, 77, 383, 1853, 3327, 1462],\n",
    "    [6601, 4237, 345, 460, 779, 284, 787, 257]\n",
    "], np.uint32)\n",
    "input_start_ids = input_start_ids.reshape([input_start_ids.shape[0], 1, input_start_ids.shape[1]])\n",
    "input_data = np.tile(input_start_ids, (1, 1, 1))\n",
    "input_len = np.array([[sentence.size] for sentence in input_start_ids], np.uint32)\n",
    "output_len = np.ones_like(input_len).astype(np.uint32) * 24\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT_ID\", \"shape\": input_data.shape, \"datatype\": np_to_triton_dtype(input_data.dtype), \n",
    "         \"data\": input_data.tolist()},\n",
    "        {\"name\": \"REQUEST_INPUT_LEN\", \"shape\": input_len.shape, \"datatype\": np_to_triton_dtype(input_len.dtype), \n",
    "         \"data\": input_len.tolist()},\n",
    "        {\"name\": \"REQUEST_OUTPUT_LEN\", \"shape\": output_len.shape, \"datatype\": np_to_triton_dtype(output_len.dtype),\n",
    "         \"data\": output_len.tolist()}\n",
    "    ]\n",
    "}\n",
    "request_parallelism = 100\n",
    "for i in range(request_parallelism):\n",
    "    predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aec3df",
   "metadata": {},
   "source": [
    "## 엔드포인트, 모델 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()\n",
    "# model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a40f3",
   "metadata": {},
   "source": [
    "## Endpoint 생성 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = \"triton-ft-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302f29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"triton-ft-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.p3.16xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eadff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"triton-ft-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aca0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "\n",
    "input_start_ids = np.array([\n",
    "    [9915, 27221, 59, 77, 383, 1853, 3327, 1462],\n",
    "    [6601, 4237, 345, 460, 779, 284, 787, 257],\n",
    "    [59, 77, 611, 7, 9248, 796, 657, 8],\n",
    "    [38, 10128, 6032, 651, 8699, 4, 4048, 20753],\n",
    "    [21448, 7006, 930, 12901, 930, 7406, 7006, 198],\n",
    "    [13256, 11, 281, 1605, 3370, 11, 1444, 6771],\n",
    "    [9915, 27221, 59, 77, 383, 1853, 3327, 1462],\n",
    "    [6601, 4237, 345, 460, 779, 284, 787, 257]\n",
    "], np.uint32)\n",
    "input_start_ids = input_start_ids.reshape([input_start_ids.shape[0], 1, input_start_ids.shape[1]])\n",
    "input_data = np.tile(input_start_ids, (1, 1, 1))\n",
    "input_len = np.array([[sentence.size] for sentence in input_start_ids], np.uint32)\n",
    "output_len = np.ones_like(input_len).astype(np.uint32) * 24\n",
    "\n",
    "from datetime import datetime\n",
    "request_parallelism = 10\n",
    "\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "stop_time = datetime.now()\n",
    "latency = ((stop_time - start_time).total_seconds()* 1000.0 / request_parallelism)\n",
    "\n",
    "for i in range(request_parallelism):\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"name\": \"INPUT_ID\", \"shape\": input_data.shape, \"datatype\": np_to_triton_dtype(input_data.dtype), \n",
    "             \"data\": input_data.tolist()},\n",
    "            {\"name\": \"REQUEST_INPUT_LEN\", \"shape\": input_len.shape, \"datatype\": np_to_triton_dtype(input_len.dtype), \n",
    "             \"data\": input_len.tolist()},\n",
    "            {\"name\": \"REQUEST_OUTPUT_LEN\", \"shape\": output_len.shape, \"datatype\": np_to_triton_dtype(output_len.dtype),\n",
    "             \"data\": output_len.tolist()}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    "    )\n",
    "\n",
    "    print(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n",
    "print(f\"[INFO] execution time: {latency} ms\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f470f",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffce8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
